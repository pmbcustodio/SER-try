{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f77db8ae-3845-46e7-8db2-2092b5fe8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pyaudio\n",
    "import wave\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df45747-7667-437a-ace1-0f69c6b2b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions in the RAVDESS dataset\n",
    "emotions = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2152339-8866-48df-bea3-2cc306c8451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions to observe\n",
    "observed_emotions = ['neutral', 'happy', 'sad', 'angry']\n",
    "# Initialize the Multi Layer Perceptron Classifier\n",
    "model = MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08,\n",
    "                      hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfba796-4db6-4a93-a859-9ac67b6be4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording user audio\n",
    "def recordAudio():\n",
    "    chunk = 1024  # Record in chunks of 1024 samples\n",
    "    sample_format = pyaudio.paInt16  # 16 bits per sample\n",
    "    channels = 1\n",
    "    fs = 48000  # Record at 44100 samples per second //as per ravdess dataset the frequecy is 48kHz\n",
    "    seconds = 5\n",
    "    filename = \"Predict-Record-Audio.wav\"\n",
    "\n",
    "    p = pyaudio.PyAudio()  # Create an interface to PortAudio\n",
    "\n",
    "    print('Recording')\n",
    "\n",
    "    stream = p.open(format=sample_format,\n",
    "                    channels=channels,\n",
    "                    rate=fs,\n",
    "                    frames_per_buffer=chunk,\n",
    "                    input=True)\n",
    "\n",
    "    frames = []  # Initialize array to store frames\n",
    "\n",
    "    # Store data in chunks for 10 seconds\n",
    "    for i in range(0, int(fs / chunk * seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    # Stop and close the stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    # Terminate the PortAudio interface\n",
    "    p.terminate()\n",
    "\n",
    "    print('Finished recording')\n",
    "\n",
    "    # Save the recorded data as a WAV file\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(p.get_sample_size(sample_format))\n",
    "    wf.setframerate(fs)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f78c39-fca5-4d45-8cb6-1c361762a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        result = np.array([])\n",
    "        if mfcc:\n",
    "            mfccs = np.mean(librosa.feature.mfcc(\n",
    "                y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result = np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma = np.mean(librosa.feature.chroma_stft(\n",
    "                S=stft, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel = np.mean(librosa.feature.melspectrogram(\n",
    "                y=X, sr=sample_rate).T, axis=0)\n",
    "            result = np.hstack((result, mel))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c249e82-820a-497c-a779-7dfce83684f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.2):\n",
    "    x, y = [], []\n",
    "    for file in glob.glob(\"C:\\\\Users\\\\paulc\\\\OneDrive\\\\Desktop\\\\SER\\\\\\speech-emotion-recognition-ravdess-data\\\\Actor_*\\\\*.wav\"):\n",
    "        file_name = os.path.basename(file)\n",
    "        emotion = emotions[file_name.split(\"-\")[2]]\n",
    "        \n",
    "        #print(\"File name = {} , emotion = {}\".format(file_name, emotion))\n",
    "        \n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac814a04-8bc5-4c7b-ad1b-beb6586b777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to train model if started program\n",
    "def trainModel():\n",
    "\n",
    "    # Split the dataset\n",
    "    x_train, x_test, y_train, y_test = load_data(test_size=0.25)\n",
    "    \n",
    "    #print(x_train)\n",
    "    #print(\"\\n\")\n",
    "    #print(x_test)\n",
    "\n",
    "    # Get the shape of the training and testing datasets\n",
    "    print((x_train.shape[0], x_test.shape[0]))\n",
    "\n",
    "    # Get the number of features extracted\n",
    "    print(f'Features extracted: {x_train.shape[1]}')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Predict for the test set\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate the accuracy of our model\n",
    "    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86d757bd-1f87-4708-a275-fe16155de730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# record your audio and predict emotion\n",
    "def record_predictAudio():\n",
    "    x_predictAudio = []\n",
    "    recordAudio() #Record audio to predict\n",
    "    file = \"C:\\\\Users\\\\paulc\\\\OneDrive\\\\Desktop\\\\SER\\\\Predict-Record-Audio.wav\" #Recorded audio filepath\n",
    "    featurePredictAudio = extract_feature(file, mfcc=True, chroma=True, mel=True) #extract features of recorded audio\n",
    "    x_predictAudio.append(featurePredictAudio)\n",
    "    y_predictAudio = model.predict(np.array(x_predictAudio))\n",
    "    print(\"Emotion Predicted: {}\".format(y_predictAudio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182fe863-40a0-4c0b-8bfa-93b47f861014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 to create and train model. \n",
      "Enter 2 to record and predict audio. \n",
      "Enter 3 to predict on pre-recorded audio. \n",
      "Enter 4 to quit. \n",
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 168)\n",
      "Features extracted: 180\n",
      "Accuracy: 74.40%\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter 1 to create and train model. \n",
      "Enter 2 to record and predict audio. \n",
      "Enter 3 to predict on pre-recorded audio. \n",
      "Enter 4 to quit. \n",
      " 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording\n",
      "Finished recording\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python310\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=2\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     trainModel()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mrecord_predictAudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m choice \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m     18\u001b[0m     predictAudio()\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mrecord_predictAudio\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m recordAudio() \u001b[38;5;66;03m#Record audio to predict\u001b[39;00m\n\u001b[0;32m      5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpaulc\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSER\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mPredict-Record-Audio.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#Recorded audio filepath\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m featurePredictAudio \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchroma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#extract features of recorded audio\u001b[39;00m\n\u001b[0;32m      7\u001b[0m x_predictAudio\u001b[38;5;241m.\u001b[39mappend(featurePredictAudio)\n\u001b[0;32m      8\u001b[0m y_predictAudio \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39marray(x_predictAudio))\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mextract_feature\u001b[1;34m(file_name, mfcc, chroma, mel)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mfcc:\n\u001b[0;32m     10\u001b[0m     mfccs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(\n\u001b[0;32m     11\u001b[0m         y\u001b[38;5;241m=\u001b[39mX, sr\u001b[38;5;241m=\u001b[39msample_rate, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmfccs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chroma:\n\u001b[0;32m     14\u001b[0m     chroma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(\n\u001b[0;32m     15\u001b[0m         S\u001b[38;5;241m=\u001b[39mstft, sr\u001b[38;5;241m=\u001b[39msample_rate)\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\numpy\\core\\shape_base.py:343\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# As a special case, dimension 0 of 1-dimensional arrays is \"horizontal\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arrs \u001b[38;5;129;01mand\u001b[39;00m arrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _nx\u001b[38;5;241m.\u001b[39mconcatenate(arrs, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 2 dimension(s)"
     ]
    }
   ],
   "source": [
    "# predict on pre-recorded audio\n",
    "def predictAudio():\n",
    "    file = input(\"Please enter path to your file.\\n\")\n",
    "    x_predictAudio = []\n",
    "    featurePredictAudio = extract_feature(file, mfcc=True, chroma=True, mel=True) #extract features of recorded audio\n",
    "    x_predictAudio.append(featurePredictAudio)\n",
    "    y_predictAudio = model.predict(np.array(x_predictAudio))\n",
    "    print(\"Emotion Predicted: {}\".format(y_predictAudio))\n",
    "\n",
    "\n",
    "while True:\n",
    "    choice = int(input(\"Enter 1 to create and train model. \\nEnter 2 to record and predict audio. \\nEnter 3 to predict on pre-recorded audio. \\nEnter 4 to quit. \\n\"))\n",
    "    if choice == 1:\n",
    "        trainModel()\n",
    "    elif choice == 2:\n",
    "        record_predictAudio()\n",
    "    elif choice == 3:\n",
    "        predictAudio()\n",
    "    else:\n",
    "        quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06f5ad-d21b-485f-a230-0b5288d81dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
